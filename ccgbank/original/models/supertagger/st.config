# an example supertagger config file (D.N. Mehay)
# change to suit your needs (e.g., replace the following paths
# with paths that point to the relevant files).

# this is a comment ("basic" means C,C & Vadas (2006)-style features).
# there is nothing else in OpenCCG, at the moment.
taggerType=basic

# this model gives priors on supertags.
#priorModel=stprior.flm
#priorModelVocab=vocab.st

# this is a Zhang Le-style MEM.
maxentModel=st.mod

# this last must be an ARPA-formatted n-gram model over supertags (with <s> and </s>)
# (7- to 9-grams work well, without much memory usage).
sequenceModel=st.lm

# tagging beam widths (first try beta1, then beta2 if that doesn't give a parse, etc.).
# give as many as you want, but keep in mind that your parser will try them all out
# (which may make it try to parse in vain -- i.e., when it just doesn't have the
# categories to do it).
betas =  0.024 0.003875 0.001225 0.0005377 0.000275 0.0000925 0.00004
#betas =  0.075 0.03 0.01 0.005 0.001

# if not using the prior model (above), you must give 'K' values (see Clark and Curran (2007)).
# the first one is for all beta values but the last. the second one is for
# the last.
firstK=20
lastK=100

# also, if you're not using the prior models, you need to specify a word-keyed tagging
# dictionary (this interacts with the 'K' values) and a POS-keyed tagging dictionary.
wDict=word.dict.min10
posDict=pos.dict.min10

# use automatic POS features? (even if you are automatically POS tagging,
# but only using single-best, set this to false).
autoPOS=true

# if autoPOS=true, you need to specify a POS config file.
posConfig=../postagger/pos.config

# tagging algorithm.  (choose from 'forward' and 'forward-backward')
# the former is faster, the latter is better.
taggingAlgorithm=forward-backward

# self-explanatory (will print certain errors and warnings, if set to 'true')
verbose=true